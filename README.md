# Informe de Seguridad, Privacidad y √âtica en Sistema Acad√©mico

**Integrantes**:
- Paolo Medrano Ter√°n ‚Äì 100%
- C√©sar Pajuelo Reyes ‚Äì 100%
- ChristianFrisancho Mayorga - 100%

## 1. Introducci√≥n 

En el contexto de un sistema acad√©mico distribuido simulado, este proyecto se enfoca en la aplicaci√≥n de t√©cnicas de privacidad y seguridad de datos sobre un conjunto sensible de registros estudiantiles, con el objetivo de explorar c√≥mo salvaguardar la privacidad de los individuos sin comprometer la utilidad anal√≠tica del dataset. Adem√°s, se analizan las implicancias √©ticas del tratamiento de la informaci√≥n, con √©nfasis en la equidad algor√≠tmica, la transparencia en los modelos y el respeto al consentimiento de los usuarios.

Este trabajo se realiza en cumplimiento con los principios de la Ley N.¬∞ 29733 de Protecci√≥n de Datos Personales del Per√∫ y el Reglamento General de Protecci√≥n de Datos (GDPR), incorporando una Evaluaci√≥n de Impacto en la Protecci√≥n de Datos (DPIA) que identifica y mitiga los riesgos a lo largo del ciclo de vida de los datos.

El sistema implementado permite visualizar indicadores clave de rendimiento acad√©mico (KPIs), generar recomendaciones personalizadas, y gestionar accesos diferenciados para distintos roles (estudiantes, docentes, supervisores y administradores), todo bajo un esquema seguro y respetuoso de la privacidad. Asimismo, se han incorporado t√©cnicas como encriptaci√≥n, hashing, anonimizaci√≥n y control de accesos, y se ha reflexionado sobre el uso √©tico y socialmente responsable de estos sistemas en el contexto peruano.
## 2. Justificaci√≥n 

La gesti√≥n acad√©mica moderna enfrenta el desaf√≠o de equilibrar el an√°lisis de datos educativos con la protecci√≥n de la privacidad de los estudiantes. En pa√≠ses como Per√∫, donde persisten brechas tecnol√≥gicas, sociales y de seguridad ciudadana, es a√∫n m√°s crucial implementar sistemas responsables que eviten la exposici√≥n de datos sensibles. Este proyecto propone una arquitectura segura, √©tica y educativa, que sirva como modelo de buenas pr√°cticas para entornos escolares y universitarios en transformaci√≥n digital.
## 3.Objetivo General 
Desarrollar un sistema acad√©mico simulado que permita aplicar t√©cnicas de privacidad diferencial, seguridad de datos y principios √©ticos, evaluando el impacto de estas medidas sobre la utilidad anal√≠tica del conjunto de datos y el rendimiento del modelo, en concordancia con los marcos regulatorios locales e internacionales.
###  3.1 Objetivos espec√≠ficos 
- Identificar los riesgos de privacidad, seguridad y √©tica asociados al uso de datos educativos.
- Implementar t√©cnicas de anonimizaci√≥n, hashing y encriptaci√≥n sobre atributos sensibles del dataset.
- Aplicar medidas de privacidad diferencial y justificar sus par√°metros con base en literatura y estudios previos.
- Evaluar el impacto de estas medidas en la calidad de los an√°lisis y visualizaciones de KPIs acad√©micos.
- Desarrollar dashboards diferenciados por rol de usuario, garantizando el principio de m√≠nimo privilegio.
- Reflexionar sobre el consentimiento, el sesgo algor√≠tmico, la equidad de acceso y el uso √©tico de los modelos predictivos en educaci√≥n.
- Dise√±ar un backend funcional y un frontend demostrativo que permita visualizar los resultados del sistema.
# Desarrollo
## 1. Identificar los riesgos de privacidad, seguridad y √©tica asociados al uso de datos educativos
Durante la primera fase del proyecto se realiz√≥ un an√°lisis exhaustivo del conjunto de datos acad√©micos, identificando atributos sensibles como el g√©nero, edad, consumo de alcohol, ausencias escolares, historial acad√©mico y situaci√≥n familiar. Estos atributos fueron clasificados seg√∫n su nivel de sensibilidad y posible impacto √©tico en caso de exposici√≥n. Se discutieron posibles escenarios de riesgo como la reidentificaci√≥n por combinaci√≥n de atributos, discriminaci√≥n algor√≠tmica y sesgo institucional, considerando el contexto educativo peruano y los riesgos de inseguridad digital y filtraci√≥n de datos por terceros.

**Este an√°lisis se encuentra reflejado en la secci√≥n de ‚ÄúMedidas de Seguridad y Protecci√≥n de Datos‚Äù del informe inicial, y fue complementado por la DPIA enlazada en el informe final.****

## 2. Implementar t√©cnicas de anonimizaci√≥n, hashing y encriptaci√≥n sobre atributos sensibles del dataset
Como se document√≥ en la primera entrega del proyecto, se aplicaron t√©cnicas concretas sobre los datos sensibles:
- Anonimizaci√≥n: Agrupaci√≥n de edades, generalizaci√≥n de profesiones parentales y recategorizaci√≥n de niveles educativos.
- Hashing: Se utiliz√≥ el algoritmo sha256 para transformar contrase√±as y campos de autenticaci√≥n.
- Encriptaci√≥n: Se cifraron los datos sensibles en tr√°nsito y en reposo utilizando AES-128.
**Estos procedimientos se implementaron en los notebooks Hashing, Encriptando y Anonimizacion dentro de la carpeta Data_Seguridad, y est√°n documentados paso a paso en Jupyter Notebooks.**

### 3. Aplicar medidas de privacidad diferencial y justificar sus par√°metros con base en literatura y estudios previos
Adicionalmente a lo mencionado en el informe parcial, se aplic√≥ ruido gaussiano espec√≠ficamente a las calificaciones acad√©micas (G3) en los dashboards, con el objetivo de reducir la posibilidad de reidentificaci√≥n indirecta sin afectar significativamente las visualizaciones generales.

El uso de ruido gaussiano es justificado por su efectividad en contextos donde se busca preservar la distribuci√≥n global de los datos, manteniendo la coherencia en promedios y percentiles para an√°lisis estad√≠sticos, especialmente √∫tiles en KPIs visuales. Esta t√©cnica es recomendada por OpenDP y se alinea con principios de privacidad diferencial aproximada ((Œµ, Œ¥)-DP).

Las decisiones sobre los par√°metros de ruido se respalda en : 
[Referencia ruido gaussiano ](https://projects.iq.harvard.edu/files/opendp/files/opendp_white_paper_11may2020.pdf)

### 4. Evaluar el impacto de estas medidas en la calidad de los an√°lisis y visualizaciones de KPIs acad√©micos
Las transformaciones aplicadas afectaron ligeramente la precisi√≥n de algunos an√°lisis, pero sin comprometer la utilidad general del sistema. Por ejemplo:

- Los KPIs como tasa de aprobaci√≥n, ausentismo vs rendimiento y boxplots por rol se mantuvieron interpretables.

- El uso de agrupaciones y anonimizaci√≥n mantuvo la representatividad de los datos.

**Esto puede verificarse en la secci√≥n ‚Äú4.4 An√°lisis Visual de KPIs Clave‚Äù de la primera entrega, donde se muestran visualizaciones y su interpretaci√≥n despu√©s del tratamiento.**

### 5. Desarrollar dashboards diferenciados por rol de usuario, garantizando el principio de m√≠nimo privilegio
Se implementaron dashboards adaptados a los siguientes roles: Estudiante, Profesor, Supervisor y Administrador. Cada uno accede solo a los datos permitidos por su rol:
- Los estudiantes solo ven su informaci√≥n.
- Los profesores acceden a sus grupos.
- Los supervisores solo a datos agregados.
- Los administradores ven todo el sistema.

**Esta l√≥gica se program√≥ usando prefijos de registro_id y est√° detallada en la secci√≥n 4.4 del informe.**

### 6. Evaluaci√≥n de Impacto en la Protecci√≥n de Datos (DPIA)
Como parte esencial de este proyecto, se ha desarrollado una Evaluaci√≥n de Impacto en la Protecci√≥n de Datos (DPIA), la cual permite identificar riesgos potenciales en el ciclo de vida de los datos, proponer medidas de mitigaci√≥n y asegurar el cumplimiento con los principios de privacidad y seguridad.
La DPIA desarrollada aborda aspectos como:
  - Prop√≥sito del procesamiento y fuentes de datos.
  - Naturaleza, alcance, contexto y riesgos asociados.
  - Medidas de seguridad adoptadas para prevenir accesos no autorizados, sesgos, o reidentificaci√≥n.
  - Cumplimiento con normativas como la Ley N¬∞ 29733 del Per√∫ y el GDPR.
El an√°lisis detallado se encuentra disponible en el siguiente enlace:
üîó [DPIA del Proyecto - Enlace al documento PDF](https://docs.google.com/document/d/14K-pkhg5VUPvi9n2ZrfCwhe4MpiyvWOk1q7cb5Pwuw8/edit?usp=sharing)

### 7. Reflexi√≥n √âtica sobre Consentimiento, Sesgo Algor√≠tmico, Equidad y Modelos Predictivos en Educaci√≥n

Como se trabaj√≥ en la primera parte del proyecto, es fundamental reflexionar sobre varios aspectos √©ticos relacionados con los datos educativos y el futuro uso de modelos predictivos, que en este caso a√∫n no se han implementado, pero podr√≠a darse sin ning√∫n problema para un an√°lisis m√°s detallado que pueda enriquecer decisiones de los docentes o supervisores.

- **Consentimiento informado**: El sistema est√° dise√±ado para que los estudiantes otorguen consentimiento expl√≠cito al registrarse, en caso ser menores de edad ser√°n sus apoderados, con informaci√≥n clara sobre los fines del procesamiento, los datos utilizados y sus derechos (derechos ARCO), en consonancia con la Ley N¬∞‚ÄØ29733 y el GDPR. Este requisito de transparencia est√° alineado con los principios de supervisi√≥n humana y respeto a la privacidad promovidos por Floridi & Cowls en su marco √©tico.
- **Sesgo algor√≠tmico**: Durante el desarrollo del sistema, se reconoci√≥ que los algoritmos predictivos pueden replicar o amplificar desigualdades existentes si no se dise√±an con cuidado, en variables como 'internet' y 'address'. Por ejemplo:
- Atributos como nivel educativo de los padres o el acceso a internet pueden sesgar las predicciones de rendimiento si no se contextualizan adecuadamente.
- Se propuso una estrategia de balanceo en el entrenamiento de modelos para evitar que subgrupos minoritarios (por sexo, tipo de direcci√≥n o estado familiar) sean desventajados en la inferencia.
- **Equidad de acceso**: Un hallazgo clave del an√°lisis de KPIs fue que solo el 10% de los estudiantes reporta tener acceso completo a recursos educativos. Esto revela una brecha digital que limita el impacto potencial de cualquier sistema predictivo. Si bien el modelo puede identificar estudiantes en riesgo, sin acceso equitativo a herramientas de mejora, las recomendaciones no se traducen en cambios reales.
Por tanto, se enfatiza que cualquier sistema predictivo debe ir acompa√±ado de pol√≠ticas institucionales que aseguren recursos y oportunidades para todos los estudiantes por igual.

- **√âtica predictiva:** Se discuti√≥ la responsabilidad institucional al utilizar predicciones sobre rendimiento acad√©mico.

**Estas reflexiones se encuentran detalladas en la secci√≥n 6 del informe.**

# Reflexi√≥n y Soluciones:

**Revisi√≥n del Modelo**: Realizar auditor√≠as peri√≥dicas de los modelos para detectar y corregir sesgos. Esto implica la implementaci√≥n de algoritmos de fairness (justicia), que busquen equilibrar las probabilidades de resultados para diferentes grupos.

**Uso de Datos Diversos y Balanceados**: Asegurarse de que los conjuntos de datos de entrenamiento sean representativos de todas las categor√≠as de estudiantes y no est√©n sesgados hacia un grupo espec√≠fico. Esto puede implicar la recolecci√≥n de datos adicionales o la modificaci√≥n de los conjuntos existentes para balancear las representaciones.

**Evaluaci√≥n de Impacto √âtico:** Evaluar c√≥mo las decisiones automatizadas del modelo afectan a los diferentes grupos de estudiantes y c√≥mo se pueden mitigar esos efectos.

## Enlace a la Presentaci√≥n de Proyecto

Puedes acceder a la presentaci√≥n del proyecto en formato Canva haciendo clic en el siguiente enlace:

[Presentaci√≥n del Proyecto](https://www.canva.com/design/DAGsnNXqpcQ/UiPH5fhOT5Ly18tZCeph1A/edit?utm_content=DAGsnNXqpcQ&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton)

# Referencias 
Utilizadas en informe y presentaci√≥n

1. **Dwork, C., et al.** (2006). *The Algorithmic Foundations of Differential Privacy*. Proceedings of the 37th Annual ACM Symposium on Theory of Computing. [https://dl.acm.org/doi/10.1145/1132516.1132519](https://dl.acm.org/doi/10.1145/1132516.1132519).

2. **OpenDP.** (2020). *Differential Privacy in Practice: OpenDP White Paper*. Harvard University. [https://projects.iq.harvard.edu/files/opendp/files/opendp_white_paper_11may2020.pdf](https://projects.iq.harvard.edu/files/opendp/files/opendp_white_paper_11may2020.pdf).

3. **UNESCO.** (2021). *Ethics of Artificial Intelligence*. Recommendation on the Ethics of AI. [https://unesdoc.unesco.org/ark:/48223/pf0000379155](https://unesdoc.unesco.org/ark:/48223/pf0000379155).

4. **Ley N¬∞ 29733 (2011).** *Ley de Protecci√≥n de Datos Personales* (Per√∫). Congreso de la Rep√∫blica del Per√∫. [https://www.leyes.congreso.gob.pe/](https://www.leyes.congreso.gob.pe/).

5. **Reglamento General de Protecci√≥n de Datos (GDPR)** (2016). *Reglamento (UE) 2016/679 del Parlamento Europeo y del Consejo*. [https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX%3A32016R0679](https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX%3A32016R0679).

6. **Floridi, L., & Cowls, J.** (2019). *A Unified Framework of Five Principles for AI Ethics*. Harvard Data Science Review. [https://hdsr.mitpress.mit.edu/pub/8k9d05s5](https://hdsr.mitpress.mit.edu/pub/8k9d05s5).

7. **IEEE.** (2019). *IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems*. IEEE Standards Association. [https://standards.ieee.org/](https://standards.ieee.org/).

8. **OECD.** (2019). *OECD Principles on Artificial Intelligence*. [https://www.oecd.org/going-digital/ai/principles/](https://www.oecd.org/going-digital/ai/principles/).

9. **NIST.** (2018). *Framework for Improving Critical Infrastructure Cybersecurity*. National Institute of Standards and Technology. [https://www.nist.gov/cyberframework](https://www.nist.gov/cyberframework).

10. **Harvard Kennedy School.** (2020). *Ethical AI: The Need for Transparency and Accountability*. [https://www.hks.harvard.edu/](https://www.hks.harvard.edu/).

